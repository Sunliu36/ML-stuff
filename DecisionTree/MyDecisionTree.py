import sklearn.metrics
from sklearn.model_selection import train_test_split
import math
import numpy as np
import pandas as pd

class Node:
    "Decision tree node"
    def __init__(self, entropy, num_samples, num_samples_per_class, predicted_class, num_errors, alpha=float("inf")):
        self.entropy = entropy
        self.num_samples = num_samples
        self.num_samples_per_class = num_samples_per_class
        self.predicted_class = predicted_class
        self.feature_index = 0
        self.threshold = 0
        self.left = None
        self.right = None
        self.num_errors = num_errors
        self.alpha = alpha

class DecisionTreeClassifier:
    def __init__(self, max_depth=4):
        self.max_depth = max_depth
        self.n_classes_ = 4  # a property to store the number of classes

    def _entropy(self, sample_y, n_classes):
        if len(sample_y) == 0:
            return 0

        entropy = 0
        for c in range(n_classes):
            p = np.sum(sample_y == c) / len(sample_y)
            if p > 0:
                entropy -= p * math.log2(p)

        return entropy

    def _feature_split(self, X, y,n_classes):
        # Returns:
        #  best_idx: Index of the feature for best split, or None if no split is found.
        #  best_thr: Threshold to use for the split, or None if no split is found.
        m = y.size
        if m <= 1:
            return None, None

        # Entropy of current node.

        best_criterion = self._entropy(y,n_classes)

        best_idx, best_thr = None, None
        # TODO: find the best split, loop through all the features, and consider all the
        # midpoints between adjacent training samples as possible thresholds. 
        # Compute the Entropy impurity of the split generated by that particular feature/threshold
        # pair, and return the pair with smallest impurity.
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                left_mask = X[:, feature_idx] <= threshold
                right_mask = X[:, feature_idx] > threshold
                if np.sum(left_mask) > 0 and np.sum(right_mask) > 0:
                    left_entropy = self._entropy(y[left_mask], n_classes)
                    right_entropy = self._entropy(y[right_mask], n_classes)
                    # Calculate the weighted average entropy
                    criterion = (np.sum(left_mask) / m) * left_entropy + (np.sum(right_mask) / m) * right_entropy
                    if criterion < best_criterion:
                        best_criterion = criterion
                        best_idx = feature_idx
                        best_thr = threshold
                        
        return best_idx, best_thr
    def _build_tree(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        correct_label_num = num_samples_per_class[predicted_class]
        num_errors = y.size - correct_label_num
        node = Node(
            entropy = self._entropy(y,self.n_classes_),
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
            num_errors=num_errors
        )

        if depth < self.max_depth:
            idx, thr = self._feature_split(X, y,self.n_classes_)
            if idx is not None:
                # TODO: Split the tree recursively according index and threshold until maximum depth is reached.
                left_mask = X[:, idx] <= thr
                right_mask = X[:, idx] > thr
                # TODO: Split the tree recursively according to the index and threshold
                node.feature_index = idx
                node.threshold = thr
                node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)
                node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)
                #pass
        return node

    def fit(self, X, Y):
        self.root = self._build_tree(X, Y)


    
    def predict(self, X):
        pred = []
        for i in range(len(X)):
            go_node = self.root
            while go_node.left:
                if X[i][go_node.feature_index] > go_node.threshold:
                    go_node = go_node.right
                else:
                    go_node = go_node.left
            pred.append(go_node.predicted_class)
        return pred
    
    def _find_leaves(self, root):
        #TODO
        ## find each node child leaves number
        if root.left is None:
            return 1
        count = self._find_leaves(root.left)
        count += self._find_leaves(root.right)
        #pass
        return count

    def _error_before_cut(self, root):
        # TODO
        ## return error before post-pruning
        if root.left is None:
            return root.num_errors
        errors = self._error_before_cut(root.left)
        errors += self._error_before_cut(root.right)
        #pass
        return errors

    def _compute_alpha(self, root):
        # TODO
        ## Compute each node alpha
        # alpha = (error after cut - error before cut) / (leaves been cut - 1)
        #pass
        if root.left is None:
            return float("inf")
        alpha = (root.num_errors - self._error_before_cut(root)) / (self._find_leaves(root) - 1)
        return alpha
    
    def _find_min_alpha(self, root):
        #minAlpha = float("inf")
        # TODO
        ## Search the Decision tree which have minimum alpha's node
        #pass
        minAlpha = self._compute_alpha(root)
        minNode = root
        if root.left is None:
            return minAlpha, minNode
        
        L_Alpha, L_node = self._find_min_alpha(root.left)
        R_Alpha, R_node = self._find_min_alpha(root.right)
        if L_Alpha < minAlpha:
            minAlpha = L_Alpha
            minNode = L_node
        if R_Alpha < minAlpha:
            minAlpha = R_Alpha
            minNode = R_node
        return minAlpha, minNode

    def _prune(self):
        # TODO
        # prune the decision tree with minimum alpha node
        alpha, node = self._find_min_alpha(self.root)
        if node != float('inf'):
            node.left = None
            node.right = None
        
    
def check_tree_structure_change(tree1, tree2):
    """
    Check if the structure of two decision trees is the same.

    Parameters:
    tree1: The first decision tree.
    tree2: The second decision tree to compare.

    Returns:
    True if the structures are different, False if they are the same.
    """
    def compare_nodes(node1, node2):
        if (node1 is None and node2 is not None) or (node1 is not None and node2 is None):
            return True
        if node1 is None and node2 is None:
            return False
        if node1.feature_index != node2.feature_index or node1.threshold != node2.threshold:
            return True
        left_different = compare_nodes(node1.left, node2.left)
        right_different = compare_nodes(node1.right, node2.right)
        return left_different or right_different

    return compare_nodes(tree1.root, tree2.root)
    
def load_train_test_data(test_ratio=.3, random_state=1):
    df = pd.read_csv('./car.data', names=['buying', 'maint',
                     'doors', 'persons', 'lug_boot', 'safety', 'target'])
    X = df.drop(columns=['target'])
    
    X = np.array(X.values)
    y = np.array(df['target'].values)
    label = np.unique(y)
    # for j in range(10) :
    #     print(X[j])
    for i in range(len(y)):
        for j in range(len(label)):
            if y[i] == label[j]:
                y[i] = j
                break

    y = y.astype('int')
    # for i in y :
    #     print(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_ratio, random_state=random_state, stratify=y)
    return X_train, X_test, y_train, y_test

def accuracy_report(X_train_scale, y_train, X_test_scale, y_test, max_depth=3):
    tree = DecisionTreeClassifier(max_depth=max_depth)
    tree.fit(X_train_scale, y_train)
    pred = tree.predict(X_train_scale)
    
    print("Tree train accuracy: %f" % (sklearn.metrics.accuracy_score(y_train, pred)))
    pred = tree.predict(X_test_scale)
    print("Tree test accuracy: %f" % (sklearn.metrics.accuracy_score(y_test, pred)))

    for i in range(20):
        print("=============Cut=============")
        tree._prune()
        # initial_tree = DecisionTreeClassifier(max_depth=7)
        # initial_tree.fit(X_train_scale, y_train)

        # pruned_tree = DecisionTreeClassifier(max_depth=7)
        # pruned_tree.fit(X_train_scale, y_train)

        # has_structure_changed = check_tree_structure_change(initial_tree, pruned_tree)
        # if has_structure_changed:
        #     print("The decision tree structure changed after pruning.")
        # else:
        #     print("The decision tree structure remained the same after pruning.")
        pred = tree.predict(X_train_scale)
        print("Tree train accuracy: %f" % (sklearn.metrics.accuracy_score(y_train, pred)))
        pred = tree.predict(X_test_scale)
        print("Tree test accuracy: %f" % (sklearn.metrics.accuracy_score(y_test, pred)))
        
def main():
   
    X_train, X_test, y_train, y_test = load_train_test_data(test_ratio=.3, random_state=1)
    
    accuracy_report(X_train, y_train, X_test, y_test, max_depth=14)
  

if __name__ == "__main__":
    main()

